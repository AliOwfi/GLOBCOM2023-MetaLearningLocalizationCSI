{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f08b50e-ab50-4b00-9ee2-7b328493fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import timeit\n",
    "from  torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6cc5e6-5ffb-41bc-9415-42c6bf63f02a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# naming convention: x0_y0_p100_b40_s1_t1.mat\n",
    "# room_scenarios = [3,4,5,8,10,11,12,13,21]\n",
    "# corridor_scenarios = [16,17,18,24,25,30,31,32,33]\n",
    "# open_scenarios = [6,7,9,14,15,19,20,26,27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9deb3a5f-d137-4bed-801d-c4abd121b478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_folder  = 'data/processed/'\n",
    "file_names = os.listdir(data_folder)\n",
    "grid_width = 60\n",
    "num_bursts_each_file =40\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdb5d658-bed0-4cc6-8746-402e1a3f381f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "079a90e8-347d-4a75-b6ed-57d6118c9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    " \n",
    "    \n",
    "class CsiMetaDataset(Dataset):\n",
    "    def __init__(self, data_folder, scenario, dataset_type, shots=None):\n",
    "         \n",
    "        if dataset_type not in ['linear', '1dConv']:\n",
    "            raise ValueError('Wrong type of dataset')\n",
    "        \n",
    "        file_names = os.listdir(data_folder)\n",
    "        self.list_samples = [] \n",
    "        \n",
    "        for file_name in file_names:\n",
    "            \n",
    "            if f't{scenario}.mat' not in file_name:\n",
    "                continue\n",
    "            x = int(file_name.split('_')[0][1:])\n",
    "            y = int(file_name.split('_')[1][1:])\n",
    "            \n",
    "            file_data = loadmat(f'{data_folder}{file_name}')\n",
    "            file_data['list_packets'] = np.real(file_data['list_packets'])\n",
    "            \n",
    "            counter = 0\n",
    "            for packet_id in range(file_data['list_packets'].shape[2]): #TODO: don't choose all packets from the start\n",
    "                \n",
    "                if shots and counter >= shots:\n",
    "                    break\n",
    "                    \n",
    "                if dataset_type == 'linear':\n",
    "                    self.list_samples.append({'csi_data' : file_data['list_packets'][:,:,packet_id].flatten(),\n",
    "                                              'coordinates': torch.tensor([x,y])})\n",
    "                elif dataset_type == '1dConv':\n",
    "                    self.list_samples.append({'csi_data' : file_data['list_packets'][:,:,packet_id],\n",
    "                                              'coordinates': torch.tensor([x,y])})\n",
    "                counter += 1\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.list_samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.list_samples[idx]\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7e4c8c25-a9bb-48d1-a1b5-8f7d434575bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_support_dataloaders = []\n",
    "train_query_dataloaders = []\n",
    "\n",
    "test_support_dataloaders = []\n",
    "test_query_dataloaders = []\n",
    "\n",
    "class DataloderClass():\n",
    "    def __init__(self, train_scenarios, test_scenarios, batch_size, shots=5):\n",
    "        self.train_support_dataloaders = []\n",
    "        self.train_query_dataloaders = []\n",
    "\n",
    "        self.test_support_dataloaders = []\n",
    "        self.test_query_dataloaders = []\n",
    "        \n",
    "        \n",
    "        for scenario in tqdm(train_scenarios):\n",
    "            dataset = CsiMetaDataset(data_folder, scenario, dataset_type='1dConv', shots=shots)\n",
    "            self.train_support_dataloaders.append(DataLoader(dataset, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "            dataset = CsiMetaDataset(data_folder, scenario, dataset_type='1dConv', shots=300)\n",
    "            self.train_query_dataloaders.append(DataLoader(dataset, batch_size=batch_size, shuffle=True))\n",
    "    \n",
    "        for scenario in test_scenarios:\n",
    "            dataset = CsiMetaDataset(data_folder, scenario, dataset_type='1dConv', shots=shots)\n",
    "            self.test_support_dataloaders.append(DataLoader(dataset, batch_size=batch_size, shuffle=True))\n",
    "\n",
    "            dataset = CsiMetaDataset(data_folder, scenario, dataset_type='1dConv', shots=300)\n",
    "            self.test_query_dataloaders.append(DataLoader(dataset, batch_size=batch_size, shuffle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ddc12b53-d513-4c56-b3e5-c46be05dba60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.1\n",
    "\n",
    "def eulicidain_distance_loss(pred, target):\n",
    "    return torch.mean(torch.sqrt(torch.sum(\n",
    "        torch.square(pred - target), axis= 1)))\n",
    "\n",
    "class CnnLocalization(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()       \n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(3,10,3,padding=1),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv1d(10,15,3,padding=1),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(105, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(8, 2),\n",
    "            \n",
    "        ) \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def functional_forward(self, x, params):\n",
    "        \n",
    "        x = F.conv1d(x, weight=params['layers.0.weight'], bias=params['layers.0.bias'], padding=1)\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.conv1d(x, weight=params['layers.3.weight'], bias=params['layers.3.bias'], padding=1)\n",
    "        x = F.max_pool1d(x, kernel_size=2)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.linear(x, weight=params['layers.7.weight'], bias=params['layers.7.bias'])\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.linear(x, weight=params['layers.9.weight'], bias=params['layers.9.bias'])\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.linear(x, weight=params['layers.11.weight'], bias=params['layers.11.bias'])\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.linear(x, weight=params['layers.13.weight'], bias=params['layers.13.bias'])\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.linear(x, weight=params['layers.15.weight'], bias=params['layers.15.bias'])\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = F.linear(x, weight=params['layers.17.weight'], bias=params['layers.17.bias'])\n",
    "        x = F.relu(x)\n",
    "#         print(x)\n",
    "        return F.linear(x, weight=params['layers.19.weight'], bias=params['layers.19.bias'])\n",
    "    \n",
    "cnn_v1 = CnnLocalization().to(device)\n",
    "# summary(cnn_v1,(3,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6e48f40f-97c7-4b5e-8e47-c6a7e5797c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_over_iteration(training_results):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.plot(np.array(training_results[0])[:,0]*60, label=\"Meta Train-Support Loss\")\n",
    "    plt.plot(np.array(training_results[0])[:,1]*60, label=\"Meta Train-Query Loss\")\n",
    "    plt.plot(np.array(training_results[1])[:,0]*60, label=\"Meta Test-Support Loss\")\n",
    "    plt.plot(np.array(training_results[1])[:,1]*60, label=\"Meta Test-Query Loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ab494ca-117c-4ebf-ba93-34402c767f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAML():\n",
    "    def __init__(self, inner_step_size:int , inner_epochs:int , outer_step_size:int,\n",
    "                  first_order:bool, scheduler_gamma=0.9,\n",
    "                inner_loop_print_frequncy=5):\n",
    "        \n",
    "        self.inner_step_size = inner_step_size\n",
    "        self.inner_epochs = inner_epochs\n",
    "        self.outer_step_size = outer_step_size\n",
    "        self.first_order = first_order\n",
    "        \n",
    "        self.inner_loop_print_frequncy = inner_loop_print_frequncy\n",
    "        \n",
    "        self.time_start = 0\n",
    "        \n",
    "        self.model = CnnLocalization().to(device)\n",
    "        self.model_params = list(self.model.parameters()) #is model_params updated?\n",
    "        \n",
    "        self.loss_func = nn.MSELoss()#eulicidain_distance_loss\n",
    "        \n",
    "        self.meta_optimizer = torch.optim.Adam(self.model.parameters(), lr=outer_step_size)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.meta_optimizer, step_size=30, gamma=scheduler_gamma)\n",
    "    \n",
    "    \n",
    "    def set_dataloader(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "    \n",
    "    def print_epoch_results(self, epoch, loss, is_train:bool):\n",
    "        \"\"\"\n",
    "        loss = [support_loss, query_loss]\n",
    "        \"\"\"\n",
    "        mode = \"Meta Training\"\n",
    "        if not is_train:\n",
    "            mode = \"Meta Test\"\n",
    "            \n",
    "        print(f\"Epoch {epoch}: {mode} Support Loss: {loss[0]:.5f}  Query Loss {loss[1]:.5f}\")\n",
    "    \n",
    "    \n",
    "    def get_runtime(self):\n",
    "        return timeit.default_timer() - self.time_start\n",
    "    \n",
    "    \n",
    "    def train(self, task_index):\n",
    "        self.model.train()\n",
    "        \n",
    "        test_loss = 0\n",
    "        mean_outer_loss = torch.tensor(0., device=device)\n",
    "        \n",
    "            \n",
    "        #Train Support phase\n",
    "        adapt_params = collections.OrderedDict(self.model.named_parameters())   \n",
    "\n",
    "        for inner_epoch in range(self.inner_epochs):    \n",
    "\n",
    "            total_inner_loss = 0\n",
    "            support_data_length = 0\n",
    "            for batch in self.dataloader.train_support_dataloaders[task_index]: #Not clean  \n",
    "                \n",
    "                support_data_length += len(batch['coordinates'])\n",
    "\n",
    "                pred = self.model.functional_forward(batch['csi_data'].type(torch.FloatTensor).to(device),\n",
    "                                                     params=adapt_params)\n",
    "\n",
    "                inner_loss = self.loss_func(pred, batch['coordinates'].type(torch.FloatTensor).to(device))\n",
    "                start = time.time()\n",
    "        \n",
    "                adapt_grad = torch.autograd.grad(inner_loss, adapt_params.values(),\n",
    "                                         create_graph= not self.first_order)\n",
    "                print(\"time ended: \", {time.time() - start})\n",
    "                adapt_params = collections.OrderedDict((name, param - self.inner_step_size * grads)\n",
    "                                               for ((name, param), grads) in zip(adapt_params.items(), adapt_grad))\n",
    "\n",
    "                total_inner_loss += inner_loss.item()*len(batch['coordinates'])\n",
    "            \n",
    "            total_inner_loss /= support_data_length\n",
    "            \n",
    "            if self.inner_loop_print_frequncy and inner_epoch % self.inner_loop_print_frequncy == 0:\n",
    "                print(\"     ------------------------------\")\n",
    "                print(f\"Epoch: {inner_epoch} Total Inner Loss : {total_inner_loss}\")\n",
    "                print(pred[:2])\n",
    "                print(\"     ------------------------------\")    \n",
    "        \n",
    "        #Train Query phase\n",
    "        query_data_length = 0\n",
    "        \n",
    "        for batch in self.dataloader.train_query_dataloaders[task_index]:\n",
    "            with torch.set_grad_enabled(True): #for training\n",
    "                test_pred = self.model.functional_forward(batch['csi_data'].type(torch.FloatTensor).to(device),\n",
    "                                                          params=adapt_params)\n",
    "                mean_outer_loss += len(batch['coordinates']) * self.loss_func(test_pred,\n",
    "                                                  batch['coordinates'].type(torch.FloatTensor).to(device))        \n",
    "                \n",
    "            query_data_length += len(batch['coordinates'])\n",
    "    \n",
    "        mean_outer_loss.div_(query_data_length)\n",
    "        \n",
    "        self.meta_optimizer.zero_grad()\n",
    "        start = time.time()\n",
    "        mean_outer_loss.backward()\n",
    "        self.meta_optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        print(\"time ended: \", {time.time() - start})\n",
    "        raise Exception()\n",
    "        \n",
    "        \n",
    "        return  total_inner_loss, mean_outer_loss.item()\n",
    "    \n",
    "    \n",
    "    def test(self, task_index):\n",
    "        self.model.train()\n",
    "        \n",
    "        test_loss = 0\n",
    "        mean_outer_loss = torch.tensor(0., device=device)\n",
    "           \n",
    "        #Test Support phase\n",
    "        adapt_params = collections.OrderedDict(self.model.named_parameters())   \n",
    "\n",
    "        for inner_epoch in range(self.inner_epochs):    \n",
    "\n",
    "            total_inner_loss = 0\n",
    "            support_data_length = 0\n",
    "            for batch in self.dataloader.test_support_dataloaders[task_index]: #Not clean  \n",
    "                \n",
    "                support_data_length += len(batch['coordinates'])\n",
    "\n",
    "                pred = self.model.functional_forward(batch['csi_data'].type(torch.FloatTensor).to(device),\n",
    "                                                     params=adapt_params)\n",
    "\n",
    "                inner_loss = self.loss_func(pred, batch['coordinates'].type(torch.FloatTensor).to(device))\n",
    "\n",
    "                adapt_grad = torch.autograd.grad(inner_loss, adapt_params.values(),\n",
    "                                         create_graph= not self.first_order)\n",
    "\n",
    "                adapt_params = collections.OrderedDict((name, param - self.inner_step_size * grads)\n",
    "                                               for ((name, param), grads) in zip(adapt_params.items(), adapt_grad))\n",
    "\n",
    "                total_inner_loss += inner_loss.item()*len(batch['coordinates'])\n",
    "            \n",
    "            total_inner_loss /= support_data_length\n",
    "            \n",
    "            if self.inner_loop_print_frequncy and inner_epoch % self.inner_loop_print_frequncy == 0:\n",
    "                print(\"      ------------------------------\")\n",
    "                print(f\"Epoch: {inner_epoch} Total Inner Loss : {total_inner_loss}\")\n",
    "                print(batch['coordinates'][:2])\n",
    "                print(\"      ------------------------------\")\n",
    "        \n",
    "        #Test Query phase\n",
    "        query_data_length = 0\n",
    "        \n",
    "        pred_gtruth_sets = []\n",
    "        \n",
    "        for batch in self.dataloader.test_query_dataloaders[task_index]:\n",
    "            test_pred = self.model.functional_forward(batch['csi_data'].type(torch.FloatTensor).to(device),\n",
    "                                                      params=adapt_params)\n",
    "            mean_outer_loss += len(batch['coordinates']) * self.loss_func(test_pred,\n",
    "                                              batch['coordinates'].type(torch.FloatTensor).to(device))        \n",
    "            \n",
    "            pred_gtruth_sets.append((test_pred, batch['coordinates'].type(torch.FloatTensor).to(device)))\n",
    "            \n",
    "            query_data_length += len(batch['coordinates'])\n",
    "\n",
    "        mean_outer_loss.div_(query_data_length)\n",
    "        \n",
    "\n",
    "        return  total_inner_loss, mean_outer_loss.item(), pred_gtruth_sets\n",
    "    \n",
    "    \n",
    "\n",
    "    def run(self, iterations, train=False, test=False):\n",
    "        self.time_start = timeit.default_timer()\n",
    "\n",
    "        list_meta_train_loss = []\n",
    "        list_meta_test_loss = []\n",
    "        list_runtimes = []\n",
    "        \n",
    "        for iteration in tqdm(range(iterations)):\n",
    "            \n",
    "            pred_gtruth_sets_all_tasks = []\n",
    "            \n",
    "            print(f\"================={iteration}=================\")\n",
    "            if train:\n",
    "                train_task_losses = []\n",
    "                \n",
    "                task_indices = list(range(0, len(self.dataloader.train_support_dataloaders), 1))\n",
    "                random.shuffle(task_indices)\n",
    "                for task_index in task_indices:\n",
    "                    \n",
    "                    train_support_loss, train_query_loss = self.train(task_index)\n",
    "                    train_task_losses.append([train_support_loss, train_query_loss])\n",
    "                    \n",
    "                    print(f\"      Epoch:{iteration} Training Task {task_index} Support Loss {train_support_loss} Query Loss {train_query_loss}\")\n",
    "                    \n",
    "                list_meta_train_loss.append(np.array(train_task_losses).mean(axis=0))\n",
    "                self.print_epoch_results(iteration, list_meta_train_loss[-1], True)\n",
    "                \n",
    "            if test:\n",
    "                \n",
    "                test_task_losses = []\n",
    "                \n",
    "                task_indices = list(range(0, len(self.dataloader.test_support_dataloaders), 1))\n",
    "                random.shuffle(task_indices)\n",
    "                \n",
    "                for task_index in task_indices:\n",
    "                    \n",
    "                    test_support_loss, test_query_loss, pred_gtruth_sets = self.test(task_index)\n",
    "                    test_task_losses.append([test_support_loss, test_query_loss])\n",
    "                    \n",
    "                    pred_gtruth_sets_all_tasks.append(pred_gtruth_sets)\n",
    "                    \n",
    "                    print(f\"      Epoch:{iteration} Testing Task {task_index} Support Loss {test_support_loss} Query Loss {test_query_loss}\")\n",
    "                    \n",
    "                list_meta_test_loss.append(np.array(test_task_losses).mean(axis=0))\n",
    "                self.print_epoch_results(iteration, list_meta_test_loss[-1], False)\n",
    "                \n",
    "            if train:\n",
    "                list_runtimes.append(self.get_runtime())\n",
    "                print(f\"Runtime: {self.get_runtime():.2f}, \\n\")\n",
    "            print(\"========================================\")\n",
    "            \n",
    "            \n",
    "        return  list_meta_train_loss, list_meta_test_loss, list_runtimes, pred_gtruth_sets_all_tasks\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a87517b-c8c1-43e2-a337-d61dbd331d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptive_MAML():\n",
    "    def __init__(self, inner_step_size:int , inner_epochs:int , outer_step_size:int,\n",
    "                 first_order:bool, scheduler_gamma=0.9,\n",
    "                 scheduler_step_size=30, inner_loop_print_frequncy=5):\n",
    "        \n",
    "        self.inner_step_size = inner_step_size\n",
    "        self.inner_epochs = inner_epochs\n",
    "        \n",
    "        self.outer_step_size = outer_step_size\n",
    "        self.iteration_step_size = self.outer_step_size\n",
    "        self.scheduler_step_size = scheduler_step_size\n",
    "        self.scheduler_gamma = scheduler_gamma\n",
    "        \n",
    "        self.first_order = first_order\n",
    "        \n",
    "        self.inner_loop_print_frequncy = inner_loop_print_frequncy\n",
    "        \n",
    "        self.time_start = 0\n",
    "        \n",
    "        self.model = CnnLocalization().to(device)\n",
    "        self.model_params = list(self.model.parameters()) #is model_params updated?\n",
    "        \n",
    "        self.loss_func = nn.MSELoss()#eulicidain_distance_loss\n",
    "        \n",
    "        self.meta_optimizer = torch.optim.Adam(self.model.parameters(), lr=outer_step_size)\n",
    "        \n",
    "        self.adaptive_step_sizes = calcualte_adaptive_step_size(center_step_size=self.outer_step_size, \n",
    "                                                                importance_vector=read_importance_vector())\n",
    "    \n",
    "    def set_dataloader(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "    \n",
    "    def print_epoch_results(self, epoch, loss, is_train:bool):\n",
    "        \"\"\"\n",
    "        loss = [support_loss, query_loss]\n",
    "        \"\"\"\n",
    "        mode = \"Meta Training\"\n",
    "        if not is_train:\n",
    "            mode = \"Meta Test\"\n",
    "            \n",
    "        print(f\"Epoch {epoch}: {mode} Support Loss: {loss[0]:.5f}  Query Loss {loss[1]:.5f}\")\n",
    "    \n",
    "    \n",
    "    def get_runtime(self):\n",
    "        return timeit.default_timer() - self.time_start\n",
    "    \n",
    "    \n",
    "    def train(self, task_index):\n",
    "        self.model.train()\n",
    "        \n",
    "        test_loss = 0\n",
    "        mean_outer_loss = torch.tensor(0., device=device)\n",
    "        \n",
    "            \n",
    "        #Train Support phase\n",
    "        adapt_params = collections.OrderedDict(self.model.named_parameters())   \n",
    "\n",
    "        for inner_epoch in range(self.inner_epochs):    \n",
    "\n",
    "            total_inner_loss = 0\n",
    "            support_data_length = 0\n",
    "            for batch in self.dataloader.train_support_dataloaders[task_index]: #Not clean  \n",
    "                \n",
    "                support_data_length += len(batch['coordinates'])\n",
    "\n",
    "                pred = self.model.functional_forward(batch['csi_data'].type(torch.FloatTensor).to(device),\n",
    "                                                     params=adapt_params)\n",
    "\n",
    "                inner_loss = self.loss_func(pred, batch['coordinates'].type(torch.FloatTensor).to(device))\n",
    "\n",
    "                adapt_grad = torch.autograd.grad(inner_loss, adapt_params.values(),\n",
    "                                         create_graph= not self.first_order)\n",
    "\n",
    "                adapt_params = collections.OrderedDict((name, param - self.inner_step_size * grads)\n",
    "                                               for ((name, param), grads) in zip(adapt_params.items(), adapt_grad))\n",
    "\n",
    "                total_inner_loss += inner_loss.item()*len(batch['coordinates'])\n",
    "            \n",
    "            total_inner_loss /= support_data_length\n",
    "            \n",
    "            if self.inner_loop_print_frequncy and inner_epoch % self.inner_loop_print_frequncy == 0:\n",
    "                print(\"     ------------------------------\")\n",
    "                print(f\"Epoch: {inner_epoch} Total Inner Loss : {total_inner_loss}\")\n",
    "                print(pred[:2])\n",
    "                print(\"     ------------------------------\")    \n",
    "        \n",
    "        #Train Query phase\n",
    "        query_data_length = 0\n",
    "        \n",
    "        for batch in self.dataloader.train_query_dataloaders[task_index]:\n",
    "            with torch.set_grad_enabled(True): #for training\n",
    "                test_pred = self.model.functional_forward(batch['csi_data'].type(torch.FloatTensor).to(device),\n",
    "                                                          params=adapt_params)\n",
    "                mean_outer_loss += len(batch['coordinates']) * self.loss_func(test_pred,\n",
    "                                                  batch['coordinates'].type(torch.FloatTensor).to(device))        \n",
    "                \n",
    "            query_data_length += len(batch['coordinates'])\n",
    "    \n",
    "        mean_outer_loss.div_(query_data_length)\n",
    "        \n",
    "        \n",
    "        for g in self.meta_optimizer.param_groups:\n",
    "            g['lr'] = self.adaptive_step_sizes[task_index]\n",
    "            \n",
    "        self.meta_optimizer.zero_grad()\n",
    "        mean_outer_loss.backward()\n",
    "        self.meta_optimizer.step()\n",
    "        \n",
    "        return  total_inner_loss, mean_outer_loss.item()\n",
    "    \n",
    "    \n",
    "    def test(self, task_index):\n",
    "        self.model.train()\n",
    "        \n",
    "        test_loss = 0\n",
    "        mean_outer_loss = torch.tensor(0., device=device)\n",
    "           \n",
    "        #Test Support phase\n",
    "        adapt_params = collections.OrderedDict(self.model.named_parameters())   \n",
    "\n",
    "        for inner_epoch in range(self.inner_epochs):    \n",
    "\n",
    "            total_inner_loss = 0\n",
    "            support_data_length = 0\n",
    "            for batch in self.dataloader.test_support_dataloaders[task_index]: #Not clean  \n",
    "                \n",
    "                support_data_length += len(batch['coordinates'])\n",
    "\n",
    "                pred = self.model.functional_forward(batch['csi_data'].type(torch.FloatTensor).to(device),\n",
    "                                                     params=adapt_params)\n",
    "\n",
    "                inner_loss = self.loss_func(pred, batch['coordinates'].type(torch.FloatTensor).to(device))\n",
    "\n",
    "                adapt_grad = torch.autograd.grad(inner_loss, adapt_params.values(),\n",
    "                                         create_graph= not self.first_order)\n",
    "\n",
    "                adapt_params = collections.OrderedDict((name, param - self.inner_step_size * grads)\n",
    "                                               for ((name, param), grads) in zip(adapt_params.items(), adapt_grad))\n",
    "\n",
    "                total_inner_loss += inner_loss.item()*len(batch['coordinates'])\n",
    "            \n",
    "            total_inner_loss /= support_data_length\n",
    "            \n",
    "            if self.inner_loop_print_frequncy and inner_epoch % self.inner_loop_print_frequncy == 0:\n",
    "                print(\"      ------------------------------\")\n",
    "                print(f\"Epoch: {inner_epoch} Total Inner Loss : {total_inner_loss}\")\n",
    "                print(batch['coordinates'][:2])\n",
    "                print(\"      ------------------------------\")\n",
    "        \n",
    "        #Test Query phase\n",
    "        query_data_length = 0\n",
    "        \n",
    "        pred_gtruth_sets = []\n",
    "        \n",
    "        for batch in self.dataloader.test_query_dataloaders[task_index]:\n",
    "            test_pred = self.model.functional_forward(batch['csi_data'].type(torch.FloatTensor).to(device),\n",
    "                                                      params=adapt_params)\n",
    "            mean_outer_loss += len(batch['coordinates']) * self.loss_func(test_pred,\n",
    "                                              batch['coordinates'].type(torch.FloatTensor).to(device)) \n",
    "            \n",
    "            pred_gtruth_sets.append((test_pred, batch['coordinates'].type(torch.FloatTensor).to(device)))\n",
    "                \n",
    "            query_data_length += len(batch['coordinates'])\n",
    "\n",
    "        mean_outer_loss.div_(query_data_length)\n",
    "        \n",
    "\n",
    "        return  total_inner_loss, mean_outer_loss.item(), pred_gtruth_sets\n",
    "    \n",
    "    \n",
    "\n",
    "    def run(self, iterations, train=False, test=False):\n",
    "        self.time_start = timeit.default_timer()\n",
    "\n",
    "        list_meta_train_loss = []\n",
    "        list_meta_test_loss = []\n",
    "        list_runtimes = []\n",
    "        \n",
    "        for iteration in tqdm(range(iterations)):\n",
    "            \n",
    "            pred_gtruth_sets_all_tasks = []\n",
    "            \n",
    "            if iteration% self.scheduler_step_size == 0:\n",
    "                self.iteration_step_size *= self.scheduler_gamma\n",
    "                self.adaptive_step_sizes = calcualte_adaptive_step_size(center_step_size=self.iteration_step_size, \n",
    "                                                                        importance_vector=read_importance_vector())\n",
    "                \n",
    "                print(self.adaptive_step_sizes)\n",
    "                \n",
    "            print(f\"================={iteration}=================\")\n",
    "            if train:\n",
    "                train_task_losses = []\n",
    "                \n",
    "                task_indices = list(range(0, len(self.dataloader.train_support_dataloaders), 1))\n",
    "                random.shuffle(task_indices)\n",
    "                for task_index in task_indices:\n",
    "                    \n",
    "                    train_support_loss, train_query_loss = self.train(task_index)\n",
    "                    train_task_losses.append([train_support_loss, train_query_loss])\n",
    "                    \n",
    "                    print(f\"      Epoch:{iteration} Training Task {task_index} Support Loss {train_support_loss} Query Loss {train_query_loss}\")\n",
    "                    \n",
    "                list_meta_train_loss.append(np.array(train_task_losses).mean(axis=0))\n",
    "                self.print_epoch_results(iteration, list_meta_train_loss[-1], True)\n",
    "                \n",
    "            if test:\n",
    "                \n",
    "                test_task_losses = []\n",
    "                \n",
    "                task_indices = list(range(0, len(self.dataloader.test_support_dataloaders), 1))\n",
    "                random.shuffle(task_indices)\n",
    "                \n",
    "                for task_index in task_indices:\n",
    "                    \n",
    "                    test_support_loss, test_query_loss, pred_gtruth_sets  = self.test(task_index)\n",
    "                    test_task_losses.append([test_support_loss, test_query_loss])\n",
    "                    \n",
    "                    pred_gtruth_sets_all_tasks.append(pred_gtruth_sets)\n",
    "                    \n",
    "                    print(f\"      Epoch:{iteration} Testing Task {task_index} Support Loss {test_support_loss} Query Loss {test_query_loss}\")\n",
    "                    \n",
    "                list_meta_test_loss.append(np.array(test_task_losses).mean(axis=0))\n",
    "                self.print_epoch_results(iteration, list_meta_test_loss[-1], False)\n",
    "                \n",
    "            if train:\n",
    "                list_runtimes.append(self.get_runtime())\n",
    "                print(f\"Runtime: {self.get_runtime():.2f}, \\n\")\n",
    "            print(\"========================================\")\n",
    "            \n",
    "            \n",
    "        return  list_meta_train_loss, list_meta_test_loss, list_runtimes, pred_gtruth_sets_all_tasks\n",
    "\n",
    "def read_importance_vector():\n",
    "    return np.average(np.load('results/conventional learning/5_shot_additional_learning.npy')*grid_width, axis=1)\n",
    "\n",
    "def calcualte_adaptive_step_size(center_step_size, importance_vector):\n",
    "    \n",
    "    importance_vector = (importance_vector - importance_vector.min())/(importance_vector.max() - importance_vector.min()) #minmax\n",
    "    importance_vector = -(importance_vector -0.5 )*2 # range(-1,1), hgiher value less important\n",
    "\n",
    "    \n",
    "    importance_vector = [np.tanh(x) for x in importance_vector]\n",
    "    adaptive_stepsizes = [center_step_size + importance/1000 for importance in importance_vector]\n",
    "    return adaptive_stepsizes\n",
    "    \n",
    "adaptive_step_sizes = calcualte_adaptive_step_size(0.002, read_importance_vector())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b0eb173-98f2-42e6-8c1b-4f946e230a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[31, 23, 14, 33],\n",
       " [9, 19, 7, 17],\n",
       " [10, 20, 7, 5],\n",
       " [22, 31, 7, 23],\n",
       " [28, 21, 14, 31]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_test_tasks(n_training_tasks, n_experiments):\n",
    "    res = []\n",
    "    for _ in range(n_experiments):\n",
    "        res.append(random.sample(list(range(1,34,1)), k=34-n_training_tasks))\n",
    "    \n",
    "    return res\n",
    "\n",
    "generate_test_tasks(n_training_tasks=30,n_experiments=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6c599e54-fe02-4b3d-b8be-2dc6389334a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_directory = 'results/category_experiment/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "213f967c-64c5-4210-816e-e8ea805472cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model, seed, shots, outer_step_size,\n",
    "                   inner_step_size, inner_epochs, scheduler_gamma,iterations,\n",
    "                  batch_size, n_experiments, train_test_scenarios, experiment_name, scheduler_step_size=10):\n",
    "    \n",
    "    set_random_seed(seed)\n",
    "    \n",
    "    train_scenarios, test_scenarios = train_test_scenarios\n",
    "    \n",
    "    df_pred_truths = pd.DataFrame(columns=['truth_x', 'truth_y', 'pred_x', 'pred_y'])\n",
    "    df_results = pd.DataFrame()\n",
    "    \n",
    "    for i in range(n_experiments):\n",
    "        \n",
    "        dataloader = DataloderClass(train_scenarios, test_scenarios, batch_size, shots)\n",
    "        \n",
    "        if model == 'MAML':\n",
    "            meta_model = MAML(inner_step_size=inner_step_size , inner_epochs = inner_epochs , outer_step_size=outer_step_size,\n",
    "                          first_order=False, scheduler_gamma=scheduler_gamma,\n",
    "                         inner_loop_print_frequncy=0)\n",
    "        elif model == 'ADAMAML':\n",
    "            meta_model = Adaptive_MAML(inner_step_size=inner_step_size , inner_epochs = inner_epochs ,\n",
    "                               outer_step_size=outer_step_size, first_order=False\n",
    "                 , scheduler_gamma=scheduler_gamma, scheduler_step_size=scheduler_step_size,\n",
    "                 inner_loop_print_frequncy=0)\n",
    "        elif model == 'FOMAML':\n",
    "            meta_model = MAML(inner_step_size=inner_step_size , inner_epochs = inner_epochs , outer_step_size=outer_step_size,\n",
    "                          first_order=True, scheduler_gamma=scheduler_gamma,\n",
    "                         inner_loop_print_frequncy=0)\n",
    "        else:\n",
    "            raise Exception(\"Wrong Model\")\n",
    "        \n",
    "        meta_model.set_dataloader(dataloader)\n",
    "        training_results = meta_model.run(iterations=iterations, train=True, test=True)\n",
    "\n",
    "        df_results[f'test_{i}_train_support'] = np.array(training_results[0])[:,0]\n",
    "        df_results[f'test_{i}_train_query'] = np.array(training_results[0])[:,1]\n",
    "        df_results[f'test_{i}_test_support'] = np.array(training_results[1])[:,0]\n",
    "        df_results[f'test_{i}_test_query'] = np.array(training_results[1])[:,1]\n",
    "        df_results[f'test_{i}_runtime'] = training_results[2]\n",
    "\n",
    "        pred_truths = training_results[3]\n",
    "        \n",
    "        for pred_truths_lvl1 in pred_truths:\n",
    "            for pred_truths_lvl2 in pred_truths_lvl1:\n",
    "                data = {'truth_x': pred_truths_lvl2[1][:,0].tolist(),\n",
    "                'truth_y': pred_truths_lvl2[1][:,1].tolist(),\n",
    "                'pred_x': pred_truths_lvl2[0][:,0].tolist(),\n",
    "                'pred_y':pred_truths_lvl2[0][:,1].tolist()\n",
    "                }\n",
    "                \n",
    "                \n",
    "                df = pd.DataFrame(data)\n",
    "                \n",
    "                if len(df['truth_x']) == len(df['pred_x']):\n",
    "                    df_pred_truths = pd.concat([df_pred_truths, df], ignore_index=True)\n",
    "\n",
    "\n",
    "    df_pred_truths['error'] = df_pred_truths.apply(lambda row:((row['truth_x']-row['pred_x'])**2 + (row['truth_y']-row['pred_y'])**2)**0.5 , axis=1)\n",
    "\n",
    "    file_name = f'/{model}_{shots}shot_{inner_step_size}stepinner_{outer_step_size}stepouter_{inner_epochs}epochinner_{batch_size}batch_{scheduler_gamma}schedulegamme_{experiment_name}_{scheduler_step_size}schedulestep{seed}seed.csv'\n",
    "    df_results.to_csv(f'{results_directory}/Losses/{file_name}', index=False)\n",
    "    df_pred_truths.to_csv(f'{results_directory}/Errors/{file_name}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91baf1cc-83c2-43dc-a6cc-68b9cd3aba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_experiment(model='FOMAML', seed=0, shots=5, inner_step_size=0.005,\n",
    "#                outer_step_size=0.002, inner_epochs=10, scheduler_gamma=0.98,\n",
    "#                batch_size=32, iterations=100, n_experiments=10, train_test_scenarios=(mixed_scenarios_train,mixed_scenarios_test), experiment_name=\"mixed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
